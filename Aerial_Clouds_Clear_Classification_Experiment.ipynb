{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgh0o4xWW4gJSoXrx+hguU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igsxf22/aerial_cloud_classification/blob/main/Aerial_Clouds_Clear_Classification_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aerial Clouds Clear Classification Experiment\n",
        "> https://github.com/igsxf22/aerial_cloud_classification\n",
        "\n",
        "\n",
        "**GOAL**: In one afternoon, create a tiny binary clouds/clear classifier from open source datasets...\n",
        "* using only publicly available open-source datasets\n",
        "* without lableing any data\n",
        "* ***generalized enough to work on variety of imagery***\n",
        "<br><br>\n",
        "\n",
        "Scenario for use is for analytics on aerial, nadir or relatively steep from variety of imagery sources\n",
        " - Outside that scenario, main limitation is that it'll be inconsistent on clear frames with forward, level cameras that capture cloudy horizons\n",
        "\n",
        "`generalization test on youtube drone video - video *not* part of the dataset`\n",
        "![GIF example](https://github.com/igsxf22/aerial_cloud_classification/raw/main/cloud_class_test_640.gif)\n",
        "<br>`source: https://www.youtube.com/watch?v=1DOc8aZDtGU`\n",
        "\n",
        "## Outline\n",
        "1. Download, extract datasets (RESISC45 and GCD)\n",
        "    - RES45 is parquets, so we'll unpack those with pandas\n",
        "    - GCD is zipped jpegs\n",
        "\n",
        "2. Reclassify the datasets into two sets: \"clouds\" and \"clear\"\n",
        "    - RES45 has 45 classes\n",
        "        - 44 land --> \"clear\"\n",
        "        - 1 cloud --> \"cloud\"\n",
        "    - GCD has 7 classes\n",
        "        - 6 clouds --> \"cloud\n",
        "        - 1 clear sky --> \"clear\"\n",
        "3. Review totals and decide on sample sizes and split into train, val, test sets\n",
        "4. Standardize arrays to 256x256x3 and copy images into ultralytics classification path structure\n",
        "5. Choose your hyperparameters and train with ultralytics CLI\n",
        "    - On yolo11n-cls with larger batch sizes, expect 50 - 100 epochs\n",
        "\n"
      ],
      "metadata": {
        "id": "myltqbqt8l__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "kr1hOJ5z2kXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing ultralytics will get all libs required\n",
        "!pip install -q ultralytics\n",
        "\n",
        "# Get RESISC45 data from HuggingFace\n",
        "!git clone https://huggingface.co/datasets/tanganke/resisc45\n",
        "\n",
        "# Download the GCD data from GoogleDrive\n",
        "# GCD_GoogleDrive_url = \"https://drive.google.com/file/d/1dsgoEQLqR3YrOMBC_hOsVEUQC7HuV2fN/view?usp=sharing\"\n",
        "# Source: https://github.com/shuangliutjnu/TJNU-Ground-based-Cloud-Dataset\n",
        "!gdown -q --id 1dsgoEQLqR3YrOMBC_hOsVEUQC7HuV2fN -O /content/GCD.zip\n",
        "!unzip -q /content/GCD.zip"
      ],
      "metadata": {
        "id": "xV-QuOL6Jsej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/define required items\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def bgr_array_from_bytes(im_bytes):\n",
        "    array = np.frombuffer(im_bytes, np.uint8)\n",
        "    im_array = cv2.imdecode(array, cv2.IMREAD_COLOR)\n",
        "    return im_array"
      ],
      "metadata": {
        "id": "Zc6tfkVGKEt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build our dirs for the merged dataset\n",
        "merged = Path(\"/content/merged\")\n",
        "merged.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "splits = \"train\", \"test\", \"val\"\n",
        "classes = \"clouds\", \"clear\"\n",
        "\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        dst = merged / split / cls\n",
        "        dst.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "-CWwCzE5Cbm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the RESISC45 Data\n",
        "\n",
        "\n",
        "RESISC45 dataset is a dataset for Remote Sensing Image Scene Classification (RESISC). It contains 31,500 RGB images of size 256Ã—256 divided into 45 scene classes, each class containing 700 images. Among its notable features, RESISC45 contains varying spatial resolution ranging from 20cm to more than 30m/px.\n",
        "\n",
        "Links:\n",
        "- https://github.com/tensorflow/datasets/blob/master/docs/catalog/resisc45.md\n",
        "- https://paperswithcode.com/dataset/resisc45\n",
        "\n",
        "![](https://production-media.paperswithcode.com/datasets/resisc.png)<br>\n"
      ],
      "metadata": {
        "id": "z9rtKnkx4rn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack RES45 data and reclassify by clouds/no clouds\n",
        "RES45_CLOUD_LABEL = 9\n",
        "\n",
        "r45 = Path(\"/content/resisc45\")\n",
        "\n",
        "train_table_path = Path(\"/content/r45_train.parquet\")\n",
        "test_table_path = Path(\"/content/r45_test.parquet\")\n",
        "\n",
        "# Get ims from parquets the long way for learning; read parq files into pd df's\n",
        "_table = pq.read_table(r45 / \"data\" / \"train-00000-of-00001.parquet\")\n",
        "pq.write_table(_table, train_table_path, compression=None)\n",
        "train_df = pd.read_parquet(train_table_path)\n",
        "\n",
        "_table = pq.read_table(r45 / \"data\" / \"test-00000-of-00001.parquet\")\n",
        "pq.write_table(_table, test_table_path, compression=None)\n",
        "test_df = pd.read_parquet(test_table_path)\n",
        "\n",
        "# # Uncomment below to see more info on the dataframes\n",
        "# print(\"\\nRes45 Train dataframe\")\n",
        "# print(train_df.info())\n",
        "# print(train_df.head(n=5))\n",
        "\n",
        "# print(\"\\nRes45 Test dataframe\")\n",
        "# print(test_df.info())\n",
        "# print(test_df.head(n=5))"
      ],
      "metadata": {
        "id": "ffz6MCpyB7nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train and test data by label == cloud or label != cloud\n",
        "train_clouds = [i for i in list(zip(train_df[\"image\"], train_df[\"label\"])) if i[1] == RES45_CLOUD_LABEL]\n",
        "train_clear = [i for i in list(zip(train_df[\"image\"], train_df[\"label\"])) if i[1] != RES45_CLOUD_LABEL]\n",
        "\n",
        "test_clouds = [i for i in list(zip(test_df[\"image\"], test_df[\"label\"])) if i[1] == RES45_CLOUD_LABEL]\n",
        "test_clear = [i for i in list(zip(test_df[\"image\"], test_df[\"label\"])) if i[1] != RES45_CLOUD_LABEL]\n",
        "\n",
        "# Look at a few ims and check to make sure we split it correctly\n",
        "# i = [im_dict, im_label]\n",
        "for i in train_clouds[::100]:\n",
        "    print(\"This should be a cloud image:\")\n",
        "    cv2_imshow(bgr_array_from_bytes(i[0][\"bytes\"]))\n",
        "\n",
        "for i in train_clear[::5000]:\n",
        "    print(\"This should be a clear image:\")\n",
        "    cv2_imshow(bgr_array_from_bytes(i[0][\"bytes\"]))\n",
        "\n",
        "for i in test_clouds[::100]:\n",
        "    print(\"This should be a cloud imag:e\")\n",
        "    cv2_imshow(bgr_array_from_bytes(i[0][\"bytes\"]))\n",
        "\n",
        "for i in test_clear[::5000]:\n",
        "    print(\"This should be a clear image:\")\n",
        "    cv2_imshow(bgr_array_from_bytes(i[0][\"bytes\"]))"
      ],
      "metadata": {
        "id": "Wp2bCVccG2bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets write to our merged dataset structure\n",
        "for i, (im_dict, label) in enumerate(train_clouds):\n",
        "    write_path = merged / \"train\" / \"clouds\" / f\"train_clouds_r45_{i}.jpg\"\n",
        "    cv2.imwrite(write_path, bgr_array_from_bytes(im_dict[\"bytes\"]))\n",
        "\n",
        "for i, (im_dict, label) in enumerate(train_clear):\n",
        "    write_path = merged / \"train\" / \"clear\" / f\"train_clear_r45_{i}.jpg\"\n",
        "    cv2.imwrite(write_path, bgr_array_from_bytes(im_dict[\"bytes\"]))\n",
        "\n",
        "for i, (im_dict, label) in enumerate(test_clouds):\n",
        "    write_path = merged / \"test\" / \"clouds\" / f\"test_clouds_r45_{i}.jpg\"\n",
        "    cv2.imwrite(write_path, bgr_array_from_bytes(im_dict[\"bytes\"]))\n",
        "\n",
        "for i, (im_dict, label) in enumerate(test_clear):\n",
        "    write_path = merged / \"test\" / \"clear\" / f\"test_clear_r45_{i}.jpg\"\n",
        "    cv2.imwrite(write_path, bgr_array_from_bytes(im_dict[\"bytes\"]))"
      ],
      "metadata": {
        "id": "8PSr4nyuJzzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the totals and see if everything got sorted\n",
        "print(\"Train data (all classes) size:\", len(train_df))\n",
        "print(\"Test data (all_classes) size:\", len(test_df))\n",
        "\n",
        "print(\"\\nReclassified and saved arrays:\")\n",
        "print(\"Train clouds:\", len(list((merged / \"train\" / \"clouds\").iterdir())))\n",
        "print(\"Train clear:\", len(list((merged / \"train\" / \"clear\").iterdir())))\n",
        "print(\"Test clouds:\", len(list((merged / \"test\" / \"clouds\").iterdir())))\n",
        "print(\"Test clear:\", len(list((merged / \"test\" / \"clear\").iterdir())))"
      ],
      "metadata": {
        "id": "-1vEYoXcK9MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Ground-Based Clouds Data\n",
        "**Source Dataset**: https://github.com/shuangliutjnu/TJNU-Ground-based-Cloud-Dataset\n",
        "\n",
        "As you can see from the above dataset, cloudy images vs clear images is 550 out of 18900 in Res45, so we need more cloudy images.\n",
        "\n",
        "As you'd expect, there's not a ton of cloudy aerial imagery, it usually defeats the purpose of studying the earth.\n",
        "\n",
        "There is however this **Ground-based** cloud classification dataset, with 6 classes or clouds and 1 for clear sky.\n",
        "\n",
        "`Ground-based Cloud Data Example`:\n",
        "![GCD Data Example](https://github.com/shuangliutjnu/TJNU-Ground-based-Cloud-Dataset/blob/main/image.jpg?raw=true)\n",
        "\n",
        "While an air-to-ground cloud dataset would be ideal, since the problem is usually clouds obscuring the entire field of view of the camera, a ground-to-sky dataset of clouds where clouds fill most of the frame, this may be enough for a small binary classification model.\n",
        "\n"
      ],
      "metadata": {
        "id": "5kijl35sLDxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the GCD data\n",
        "gcd = Path(\"/content/GCD\")\n",
        "\n",
        "gcd_counts = {\"train\": 0, \"test\": 0, \"train_clouds\": 0, \"train_clear\": 0, \"test_clouds\": 0, \"test_clear\": 0}\n",
        "\n",
        "for i in gcd.iterdir():\n",
        "    print(i.name)\n",
        "\n",
        "    for j in i.iterdir():\n",
        "        print(\"\\t\", j.name, len(list(j.iterdir())))\n",
        "\n",
        "        if i.name == \"train\":\n",
        "            gcd_counts[\"train\"] += len(list(j.iterdir()))\n",
        "\n",
        "        elif i.name == \"test\":\n",
        "            gcd_counts[\"test\"] += len(list(j.iterdir()))\n",
        "\n",
        "        if i.name == \"train\" and j.name == \"4_clearsky\":\n",
        "            gcd_counts[\"train_clear\"] += len(list(j.iterdir()))\n",
        "\n",
        "        if i.name == \"train\" and j.name != \"4_clearsky\":\n",
        "            gcd_counts[\"train_clouds\"] += len(list(j.iterdir()))\n",
        "\n",
        "        if i.name == \"test\" and j.name == \"4_clearsky\":\n",
        "            gcd_counts[\"test_clear\"] += len(list(j.iterdir()))\n",
        "\n",
        "        if i.name == \"test\" and j.name != \"4_clearsky\":\n",
        "            gcd_counts[\"test_clouds\"] += len(list(j.iterdir()))\n",
        "\n",
        "print(\"\\nGCD path totals:\")\n",
        "print(\"Total train:\", gcd_counts[\"train\"], \"Total test:\", gcd_counts[\"test\"])\n",
        "print(\"Train clouds:\", gcd_counts[\"train_clouds\"], \"Train clear:\", gcd_counts[\"train_clear\"])\n",
        "print(\"Test clouds:\", gcd_counts[\"test_clouds\"], \"Test clear:\", gcd_counts[\"test_clear\"])"
      ],
      "metadata": {
        "id": "jo39g8jMVPwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCD Data is already saved as jpegs so we won't have to unpack parquets with pandas\n",
        "# So lets start reclassifying to clouds / clear\n",
        "\n",
        "sorted_counts = {\"train_clouds\": 0, \"train_clear\": 0, \"test_clouds\": 0, \"test_clear\": 0}\n",
        "\n",
        "# Split train by clouds / clear, save to merged dataset\n",
        "for folder in Path(gcd / 'train').iterdir():\n",
        "    for i, img_path in enumerate(list(folder.iterdir())):\n",
        "\n",
        "        if folder.name == '4_clearsky':\n",
        "            write_path = merged / \"train\" / \"clear\" / f\"train_clear_gcd_{img_path.name}\"\n",
        "            array = cv2.imread(img_path)\n",
        "            cv2.imwrite(write_path, cv2.resize(array, (256, 256)))\n",
        "            sorted_counts[\"train_clear\"] += 1\n",
        "\n",
        "        else:\n",
        "            write_path = merged / \"train\" / \"clouds\" / f\"train_clouds_gcd_{img_path.name}\"\n",
        "            array = cv2.imread(img_path)\n",
        "            cv2.imwrite(write_path, cv2.resize(array, (256, 256)))\n",
        "            sorted_counts[\"train_clouds\"] += 1\n",
        "\n",
        "# Same for test\n",
        "for folder in Path(gcd / 'test').iterdir():\n",
        "    for i, img_path in enumerate(list(folder.iterdir())):\n",
        "\n",
        "        if folder.name == '4_clearsky':\n",
        "            write_path = merged / \"test\" / \"clear\" / f\"test_clear_gcd_{img_path.name}\"\n",
        "            array = cv2.imread(img_path)\n",
        "            cv2.imwrite(write_path, cv2.resize(array, (256, 256)))\n",
        "            sorted_counts[\"test_clear\"] += 1\n",
        "\n",
        "        else:\n",
        "            write_path = merged / \"test\" / \"clouds\" / f\"test_clouds_gcd_{img_path.name}\"\n",
        "            array = cv2.imread(img_path)\n",
        "            cv2.imwrite(write_path, cv2.resize(array, (256, 256)))\n",
        "            sorted_counts[\"test_clouds\"] += 1"
      ],
      "metadata": {
        "id": "hcLnL_0XVdtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset totals so far\n",
        "total_merged_dataset = len(list((merged / \"train\" / \"clouds\").iterdir())) + len(list((merged / \"train\" / \"clear\").iterdir()))\n",
        "print(\"\\nTotal merged dataset:\", total_merged_dataset)\n",
        "\n",
        "for i in merged.iterdir():\n",
        "    print(\"\\t\", i.name)\n",
        "    for j in i.iterdir():\n",
        "        print(\"\\t\\t\", j.name, len(list(j.iterdir())))\n"
      ],
      "metadata": {
        "id": "AnvX_2aYV_0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample and Split Data for Training"
      ],
      "metadata": {
        "id": "MKtFXV7hMCBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Create a sampled dataset\n",
        "# We probably don't need all these images for a prototype, lets just go with 25% of it\n",
        "# And we need to borrow images from the train set to make a val set\n",
        "\n",
        "# We'll collect paths and then write into a yolo style path structure for classification training\n",
        "\n",
        "# clear existing to resample\n",
        "!rm -rf /content/sampled\n",
        "\n",
        "# Create new data set root\n",
        "sampled = Path(\"/content/sampled\")\n",
        "\n",
        "# Copy our merged tree paths\n",
        "for i in merged.iterdir():\n",
        "    for j in i.iterdir():\n",
        "        copy_path = sampled / i.name / j.name\n",
        "        copy_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(copy_path)\n",
        "\n",
        "sample_sizes = {\"train_clouds\": 3000, \"train_clear\": 4000, \"test_clouds\": 500, \"test_clear\": 600}\n",
        "\n",
        "sampled_train_clouds = random.sample(list((merged / \"train\" / \"clouds\").iterdir()), sample_sizes[\"train_clouds\"])\n",
        "train_clouds, val_clouds = train_test_split(sampled_train_clouds, test_size=0.15, random_state=42)\n",
        "\n",
        "sampled_train_clear = random.sample(list((merged / \"train\" / \"clear\").iterdir()), sample_sizes[\"train_clear\"])\n",
        "train_clear, val_clear = train_test_split(sampled_train_clear, test_size=0.15, random_state=42)\n",
        "\n",
        "sampled_test_clouds = random.sample(list((merged / \"test\" / \"clouds\").iterdir()), sample_sizes[\"test_clouds\"])\n",
        "sampled_test_clear = random.sample(list((merged / \"test\" / \"clear\").iterdir()), sample_sizes[\"test_clear\"])\n",
        "\n",
        "print(\"\\nTrain clouds:\", len(train_clouds), \"Val clouds:\", len(val_clouds))\n",
        "print(\"\\nTrain clear:\", len(train_clear), \"Val clear:\", len(val_clear))\n",
        "print(\"\\nTest clouds:\", len(sampled_test_clouds), \"Test clear:\", len(sampled_test_clear))\n",
        "\n",
        "for i in train_clouds:\n",
        "    shutil.copy(i, sampled / \"train\" / \"clouds\")\n",
        "\n",
        "for i in train_clear:\n",
        "    shutil.copy(i, sampled / \"train\" / \"clear\")\n",
        "\n",
        "for i in val_clouds:\n",
        "    shutil.copy(i, sampled / \"val\" / \"clouds\")\n",
        "\n",
        "for i in val_clear:\n",
        "    shutil.copy(i, sampled / \"val\" / \"clear\")\n",
        "\n",
        "for i in sampled_test_clouds:\n",
        "    shutil.copy(i, sampled / \"test\" / \"clouds\")\n",
        "\n",
        "for i in sampled_test_clear:\n",
        "    shutil.copy(i, sampled / \"test\" / \"clear\")"
      ],
      "metadata": {
        "id": "DzvnZ99pe8ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with Ultralytics YOLOv11"
      ],
      "metadata": {
        "id": "zVnn2RwuMM-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your hyperparameters and train\n",
        "!yolo classify train data=/content/sampled model=yolo11n-cls.pt epochs=256 imgsz=256 batch=64 patience=64 cache=True"
      ],
      "metadata": {
        "id": "5OMzhoxwlvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a random sampled test mosaic"
      ],
      "metadata": {
        "id": "pErJXpJgMRik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a visual test mosaic with 8x8 image grid\n",
        "# Repeat to try a new sample from the merged data test images\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "test_clouds_path = Path(\"/content/merged/test/clouds\")\n",
        "test_clear_path = Path(\"/content/merged/test/clear\")\n",
        "\n",
        "random_samples_clouds = random.sample(list(test_clouds_path.iterdir()), 8)\n",
        "random_samples_clear = random.sample(list(test_clear_path.iterdir()), 8)\n",
        "\n",
        "cloud_ims = [cv2.imread(str(x)) for x in random_samples_clouds]\n",
        "clear_ims = [cv2.imread(str(x)) for x in random_samples_clear]\n",
        "\n",
        "mosaic_ims = []\n",
        "\n",
        "for i, im in enumerate(cloud_ims):\n",
        "    results = model(im, verbose=False)\n",
        "    for result in results:\n",
        "        plot = result.plot().copy()\n",
        "        cv2.putText(plot, \"True: CLOUDS\", (100, 20), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "        cv2.putText(plot, \"True: CLOUDS\", (100, 20), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "        if  model.names[result.probs.top1] == \"clouds\":\n",
        "            cv2.putText(plot, \"PASS\", (150, 40), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "            cv2.putText(plot, \"PASS\", (150, 40), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "        else:\n",
        "            cv2.putText(plot, \"FAIL\", (150, 40), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "            cv2.putText(plot, \"FAIL\", (150, 40), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "        mosaic_ims.append(plot)\n",
        "\n",
        "for i, im in enumerate(clear_ims):\n",
        "    results = model(im, verbose=False)\n",
        "    for result in results:\n",
        "        plot = result.plot().copy()\n",
        "        cv2.putText(plot, \"True: CLEAR\", (100, 20), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "        cv2.putText(plot, \"True: CLEAR\", (100, 20), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "        if model.names[result.probs.top1] == \"clear\":\n",
        "            cv2.putText(plot, \"PASS\", (150, 40), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "            cv2.putText(plot, \"PASS\", (150, 40), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "        else:\n",
        "            cv2.putText(plot, \"FAIL\", (150, 40), 1, 1.25, (0,0,0), 2, cv2.LINE_AA)\n",
        "            cv2.putText(plot, \"FAIL\", (150, 40), 1, 1.25, (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "        mosaic_ims.append(plot)\n",
        "\n",
        "# Create 4 x 4 mosaic\n",
        "mosaic = np.zeros((1024, 1024, 3), dtype=np.uint8)\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        mosaic[i*256:(i+1)*256, j*256:(j+1)*256] = mosaic_ims[i*4+j]\n",
        "cv2_imshow(mosaic)\n",
        "cv2.imwrite(\"/content/mosaic.jpg\", mosaic)"
      ],
      "metadata": {
        "id": "O_-DLvnliKUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JIg7NezQSZJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}